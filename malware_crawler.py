
import sys
import hashlib
import re
import os
from multiprocessing import Process, current_process ,Queue, Pool
import threading
import socket
import datetime
import time
import urllib
import urllib.request
import zipfile
import json
import hashlib
import shutil
from urllib.request import Request, urlopen

from requests import get
from pyunpack import Archive
import filetype
from bs4 import BeautifulSoup
import requests

import craw_comm as c_c
import unzip_manager as u_m

##################READ#######################
# 실행 이전에 craw_comm에서 악성코드 수집 경로 설정할 것
#############################################

if not os.path.exists(c_c.PATH): os.makedirs(c_c.PATH)
if not os.path.exists(c_c.PATH_OTHERS): os.makedirs(c_c.PATH_OTHERS)
if not os.path.exists(c_c.PATH_TEMP): os.makedirs(c_c.PATH_TEMP)

def parse(url):
    response=requests.get(url)
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')
    return soup



def get_sample_sha256(malware_data):
    try:sha256=hashlib.sha256(malware_data).hexdigest()
    except:sha256=hashlib.sha256(malware_data.encode()).hexdigest()

    return sha256


def create_sample_temp_path(sha256,malware_data):
    sample_name=os.path.join(c_c.PATH_TEMP,sha256)
    with open(sample_name,'wb') as file_handle:
        file_handle.write(malware_data)
        
    return sample_name

def run_get_sample(malware_data,password=None):
    sha256=get_sample_sha256(malware_data)
    temp_sample_path=create_sample_temp_path(sha256,malware_data)
    if not os.path.isfile(temp_sample_path):
        return
    kind=filetype.guess(temp_sample_path)
    if kind!=None and kind.extension=='zip':
        u_m.unzip_sample(temp_sample_path,_password=password)


def malc0de():
    soup=parse('http://malc0de.com/rss')
    mlc=[row for row in soup('description')][1:]
    mlc_sites = list()
    for row in mlc:
        site = re.sub('&', '&', str(row).split()[1]).replace(',', '')
        mlc_sites.append(site)

    for url in mlc_sites:
        if not re.match('http', url):url = 'http://' + url
        try:
            response=get(url)
            malware_data = response.content
        except:
            continue
        
        run_get_sample(malware_data)


def dasmalwerk():
    url="https://das-malwerk.herokuapp.com/"

    soup=parse(url)
    links = soup.select("tbody > tr > td > a")
    for link in links:
        malware_url = link.attrs['href']
        if 'virustotal' in malware_url:continue

        try:
            response = get(malware_url)
            malware_data = response.content
        except:
            continue

        run_get_sample(malware_data,b'infected')

def urlhaus():

    request_header = {
        'Accept': 'text/html, application/xhtml+xml, image/jxr, */*',
        'Accept-Language': 'ko-KR',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'close'}

    url_list="https://urlhaus.abuse.ch/browse/page/0","https://urlhaus.abuse.ch/browse/page/1"
    default_path="https://urlhaus.abuse.ch"
    for url in url_list:
        soup=parse(url)

        #time.sleep(3)
        links = soup.select("tbody > tr > td > a")
        for link in links:
            href=link.attrs['href']
            if 'url' in href:
                child_url_link=default_path+href

                time.sleep(3)
                child_soup=parse(child_url_link)
                links = child_soup.select("tbody > tr > td > span")[0]
                malware_url=links.string

                try:
                    response = get(malware_url)
                    malware_data = response.content
                except:
                    continue

                run_get_sample(malware_data,b'infected')



def bazaar():

    ZIP_PASSWORD = b"infected"

    default_path="https://mb-api.abuse.ch/downloads/"
    soup=parse(default_path)

    links = soup.select("table > tr > td > a")

    for link in links:
        zip_name=link.attrs['href']
        if len(zip_name)<3:continue

        child_url_link=default_path+zip_name
        try:
            response = get(child_url_link)
            malware_data = response.content
        except:
            continue

        sha256=get_sample_sha256(malware_data)
        temp_sample_path=create_sample_temp_path(sha256,malware_data)

        run_get_sample(malware_data,b'infected')


"""
    malc0de_proc = Process(target=malc0de, args=())
수집대상 URL
https://urlhaus.abuse.ch/browse/
http://www.virusign.com/ -> 회원가입 필요 ㅄ
http://contagiominidump.blogspot.com/2018/ -> 다운로드 후 unzip 필요
https://dasmalwerk.eu/ -> 다운로드 후 unzip 필요 -> 패스워드 infected
"""


def run_main():
    print("Main Craw Starting")
    while True:
        u_m.unzip_process()
        bazaar()
        dasmalwerk()
        malc0de()
        urlhaus()
        
        time.sleep(100)

if __name__=="__main__":
    run_main()